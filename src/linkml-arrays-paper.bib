@book{directorate-generalforresearchandinnovationeuropeancommissionEOSCInteroperabilityFramework2021,
  title = {{{EOSC}} Interoperability Framework: Report from the {{EOSC Executive Board Working Groups FAIR}} and {{Architecture}}},
  shorttitle = {{{EOSC}} Interoperability Framework},
  author = {{Directorate-General for Research {and} Innovation (European Commission)} and EOSC Executive Board and Corcho, Oscar and Eriksson, Magnus and Kurowski, Krzysztof and Ojster{\v s}ek, Milan and Choirat, Christine and van de Sanden, Mark and Coppens, Frederik},
  year = {2021},
  publisher = {Publications Office of the European Union},
  address = {LU},
  url = {https://data.europa.eu/doi/10.2777/620649},
  urldate = {2023-03-27},
  abstract = {This document has been developed by the Interoperability Task Force of the EOSC Executive Board FAIR Working Group, with participation from the Architecture WG. Achieving interoperability within EOSC is essential in order for the federation of services that will compose EOSC to provide added value for service users. In the context of the FAIR principles, interoperability is discussed in relation to the fact that ``research data usually need to be integrated with other data; in addition, the data need to interoperate with applications or workflows for analysis, storage, and processing''. Our view on interoperability does not only consider data but also the many other research artefacts that may be used in the context of research activity, such as software code, scientific workflows, laboratory protocols, open hardware designs, etc. It also considers the need to make services and e-infrastructures as interoperable as possible. This document identifies the general principles that should drive the creation of the EOSC Interoperability Framework (EOSC IF), and organises them into the four layers that are commonly considered in other interoperability frameworks (e.g., the European Interoperability Framework - EIF): technical, semantic, organisational and legal interoperability. For each of these layers, a catalogue of problems and needs, as well as challenges and high-level recommendations have been proposed, which should be considered in the further development and implementation of the EOSC IF components. Such requirements and recommendations have been developed after an extensive review of related literature as well as by running interviews with stakeholders from ERICs (European Research Infrastructure Consortia), ESFRI (European Strategy Forum on Research Infrastructures) projects, service providers and research communities. Some examples of such requirements are: ``every semantic artefact that is being maintained in EOSC must have sufficient associated documentation, with clear examples of usage and conceptual diagrams'', or ``Coarse-grained and fine-grained dataset (and other research object) search tools need to be made available'', etc. The document finally contains a proposal for the management of FAIR Digital Objects in the context of EOSC and a reference architecture for the EOSC Interoperability Framework that is inspired by and extends the European Interoperability Reference Architecture (EIRA), identifying the main building blocks required.},
  isbn = {978-92-76-28949-4},
  langid = {english},
  lccn = {KI-02-21-055-EN-N}
}

@article{kallinikosTheoryDigitalObjects2010,
  title = {A Theory of Digital Objects},
  author = {Kallinikos, Jannis and Aaltonen, Aleksi and Marton, Attila},
  year = {2010},
  month = jun,
  journal = {First Monday},
  issn = {1396-0466},
  doi = {10.5210/fm.v15i6.3033},
  url = {https://firstmonday.org/ojs/index.php/fm/article/view/3033},
  urldate = {2023-03-31},
  abstract = {Digital objects are marked by a limited set of variable yet generic attributes such as editability, interactivity, openness and distributedness. As digital objects diffuse throughout the institutional fabric, these attributes and the information-based operations and procedures out of which they are sustained install themselves at the heart of social practice. The entities and processes that constitute the stuff of social practice are thereby rendered increasingly unstable and transfigurable, producing a context of experience in which the certainties of recurring and recognizable objects are on the wane. These claims are supported with reference to 1) the elusive identity of digital documents and the problems of authentication/preservation of records such an identity posits and 2) the operations of search engines and the effects digital search has on the content of the documents it retrieves.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {critai,critical technology},
  file = {/Users/jonny/Zotero/storage/Y3MSN3KQ/kallinikos_2010_a_theory_of_digital_objects.pdf}
}

@article{kinneySemanticScholarOpen2023,
  title = {The {{Semantic Scholar Open Data Platform}}},
  author = {Kinney, Rodney and Anastasiades, Chloe and Authur, Russell and Beltagy, Iz and Bragg, Jonathan and Buraczynski, Alexandra and Cachola, Isabel and Candra, Stefan and Chandrasekhar, Yoganand and Cohan, Arman and Crawford, Miles and Downey, Doug and Dunkelberger, Jason and Etzioni, Oren and Evans, Rob and Feldman, Sergey and Gorney, Joseph and Graham, David and Hu, Fangzhou and Huff, Regan and King, Daniel and Kohlmeier, Sebastian and Kuehl, Bailey and Langan, Michael and Lin, Daniel and Liu, Haokun and Lo, Kyle and Lochner, Jaron and MacMillan, Kelsey and Murray, Tyler and Newell, Chris and Rao, Smita and Rohatgi, Shaurya and Sayre, Paul and Shen, Zejiang and Singh, Amanpreet and Soldaini, Luca and Subramanian, Shivashankar and Tanaka, Amber and Wade, Alex D. and Wagner, Linda and Wang, Lucy Lu and Wilhelm, Chris and Wu, Caroline and Yang, Jiangjiang and Zamarron, Angele and Van Zuylen, Madeleine and Weld, Daniel S.},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2301.10140},
  url = {https://arxiv.org/abs/2301.10140},
  urldate = {2023-03-15},
  abstract = {The volume of scientific output is creating an urgent need for automated tools to help scientists keep up with developments in their field. Semantic Scholar (S2) is an open data platform and website aimed at accelerating science by helping scholars discover and understand scientific literature. We combine public and proprietary data sources using state-of-the-art techniques for scholarly PDF content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper-authorship edges, and 2.4B+ citation edges. The graph includes advanced semantic features such as structurally parsed text, natural language summaries, and vector embeddings. In this paper, we describe the components of the S2 data processing pipeline and the associated APIs offered by the platform. We will update this living document to reflect changes as we add new data offerings and improve existing services.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation and Language (cs.CL),Digital Libraries (cs.DL),FOS: Computer and information sciences}
}

@article{pettiFrictionlessDataStandards2022,
  title = {Frictionless {{Data Standards}}},
  author = {Petti, Sara and Karev, Evgeny},
  year = {2022},
  month = oct,
  journal = {Commonplace},
  doi = {10.21428/6ffd8432.1619ad93},
  url = {https://commonplace.knowledgefutures.org/pub/8x7oeawa/release/1},
  urldate = {2024-11-01},
  abstract = {Getting insight from data is not always a straightforward process. Data is often hard to find, archived in difficult to use formats, poorly structured and/or incomplete. These issues create friction and make it difficult-to-use, publish and share data. The Frictionless Data initiative at Open Knowledge Foundation aims to reduce friction in working with data, with a goal to make it effortless to transport data among different tools and platforms for further analysis. The corresponding toolkit consists of a set of standards for data and metadata interoperability, accompanied by a collection of open source software that implement these standards, and a range of best practices for data management. In this article we are going to focus on data standards, why they are important, and, using the Frictionless Data standards as a base for the reflection, we will try to understand what makes - in our opinion- a standard successful.},
  langid = {english},
  keywords = {standards},
  file = {/Users/jonny/Zotero/storage/7FX5U7AK/Petti and Karev - 2022 - Frictionless Data Standards.pdf}
}

@article{rubelNeurodataBordersEcosystem2022,
  title = {The {{Neurodata Without Borders}} Ecosystem for Neurophysiological Data Science},
  author = {R{\"u}bel, Oliver and Tritt, Andrew and Ly, Ryan and Dichter, Benjamin K and Ghosh, Satrajit and Niu, Lawrence and Baker, Pamela and Soltesz, Ivan and Ng, Lydia and Svoboda, Karel and Frank, Loren and Bouchard, Kristofer E},
  editor = {Colgin, Laura L and Jadhav, Shantanu P},
  year = {2022},
  month = oct,
  journal = {eLife},
  volume = {11},
  pages = {e78362},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.78362},
  url = {https://doi.org/10.7554/eLife.78362},
  urldate = {2023-05-30},
  abstract = {The neurophysiology of cells and tissues are monitored electrophysiologically and optically in diverse experiments and species, ranging from flies to humans. Understanding the brain requires integration of data across this diversity, and thus these data must be findable, accessible, interoperable, and reusable (FAIR). This requires a standard language for data and metadata that can coevolve with neuroscience. We describe design and implementation principles for a language for neurophysiology data. Our open-source software (Neurodata Without Borders, NWB) defines and modularizes the interdependent, yet separable, components of a data language. We demonstrate NWB's impact through unified description of neurophysiology data across diverse modalities and species. NWB exists in an ecosystem, which includes data management, analysis, visualization, and archive tools. Thus, the NWB data language enables reproduction, interchange, and reuse of diverse neurophysiology data. More broadly, the design principles of NWB are generally applicable to enhance discovery across biology through data FAIRness.},
  archive = {https://web.archive.org/web/20230530232540/https://elifesciences.org/articles/78362},
  keywords = {archive,archived,data ecosystem,data language,data standard,FAIR data,Neurophysiology,nwb}
}

@article{rubelNWB20Accessible2019,
  title = {{{NWB}}:{{N}} 2.0: {{An Accessible Data Standard}} for {{Neurophysiology}}},
  shorttitle = {{{NWB}}},
  author = {R{\"u}bel, Oliver and Tritt, Andrew and Dichter, Benjamin and Braun, Thomas and Cain, Nicholas and Clack, Nathan and Davidson, Thomas J. and Dougherty, Max and {Fillion-Robin}, Jean-Christophe and Graddis, Nile and Grauer, Michael and Kiggins, Justin T. and Niu, Lawrence and Ozturk, Doruk and Schroeder, William and Soltesz, Ivan and Sommer, Friedrich T. and Svoboda, Karel and Lydia, Ng and Frank, Loren M. and Bouchard, Kristofer},
  year = {2019},
  month = jan,
  journal = {bioRxiv},
  doi = {10.1101/523035},
  url = {https://www.biorxiv.org/content/10.1101/523035v1},
  urldate = {2019-08-30},
  abstract = {Neurodata Without Borders: Neurophysiology (NWB:N) is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build common analysis tools for neurophysiology data. With NWB:N version 2.0 (NWB:N 2.0) we made significant advances towards creating a usable standard, software ecosystem, and vibrant community for standardizing neurophysiology data. In this manuscript we focus in particular on the NWB:N data standard schema and present advances towards creating an accessible data standard for neurophysiology.},
  copyright = {{\copyright} 2019, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/jonny/Zotero/storage/XKZHXKZ9/523035v1.html}
}

@article{subashComparisonNeuroelectrophysiologyDatabases2023,
  title = {A Comparison of Neuroelectrophysiology Databases},
  author = {Subash, Priyanka and Gray, Alex and Boswell, Misque and Cohen, Samantha L. and Garner, Rachael and Salehi, Sana and Fisher, Calvary and Hobel, Samuel and Ghosh, Satrajit and Halchenko, Yaroslav and Dichter, Benjamin and Poldrack, Russell A. and Markiewicz, Chris and Hermes, Dora and Delorme, Arnaud and Makeig, Scott and Behan, Brendan and Sparks, Alana and Arnott, Stephen R. and Wang, Zhengjia and Magnotti, John and Beauchamp, Michael S. and Pouratian, Nader and Toga, Arthur W. and Duncan, Dominique},
  year = {2023},
  month = oct,
  journal = {Scientific Data},
  volume = {10},
  number = {1},
  pages = {719},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-023-02614-0},
  url = {https://www.nature.com/articles/s41597-023-02614-0},
  urldate = {2024-11-01},
  abstract = {As data sharing has become more prevalent, three pillars - archives, standards, and analysis tools - have emerged as critical components in facilitating effective data sharing and collaboration. This paper compares four freely available intracranial neuroelectrophysiology data repositories: Data Archive for the BRAIN Initiative (DABI), Distributed Archives for Neurophysiology Data Integration (DANDI), OpenNeuro, and Brain-CODE. The aim of this review is to describe archives that provide researchers with tools to store, share, and reanalyze both human and non-human neurophysiology data based on criteria that are of interest to the neuroscientific community. The Brain Imaging Data Structure (BIDS) and Neurodata Without Borders (NWB) are utilized by these archives to make data more accessible to researchers by implementing a common standard. As the necessity for integrating large-scale analysis into data repository platforms continues to grow within the neuroscientific community, this article will highlight the various analytical and customizable tools developed within the chosen archives that may advance the field of neuroinformatics.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Biophysical models,Databases},
  file = {/Users/jonny/Zotero/storage/W6ZPWLWE/Subash et al. - 2023 - A comparison of neuroelectrophysiology databases.pdf}
}

@inproceedings{trittHDMFHierarchicalData2019,
  title = {{{HDMF}}: {{Hierarchical Data Modeling Framework}} for {{Modern Science Data Standards}}},
  shorttitle = {{{HDMF}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Tritt, Andrew J. and R{\"u}bel, Oliver and Dichter, Benjamin and Ly, Ryan and Kang, Donghe and Chang, Edward F. and Frank, Loren M. and Bouchard, Kristofer},
  year = {2019},
  month = dec,
  pages = {165--179},
  doi = {10.1109/BigData47090.2019.9005648},
  url = {https://ieeexplore.ieee.org/abstract/document/9005648?casa\_token=QFyjuCR\_GWsAAAAA:kkOB89nilguUVnQIy\_tI9xGxFMKz3\_s8Vg6NtCKDmNj8BSrV\_aYchohLHyeDLBadnfwsubS46Q},
  urldate = {2024-04-20},
  abstract = {A ubiquitous problem in aggregating data across different experimental and observational data sources is a lack of software infrastructure that enables flexible and extensible standardization of data and metadata. To address this challenge, we developed HDMF, a hierarchical data modeling framework for modern science data standards. With HDMF, we separate the process of data standardization into three main components: (1) data modeling and specification, (2) data I/O and storage, and (3) data interaction and data APIs. To enable standards to support the complex requirements and varying use cases throughout the data life cycle, HDMF provides object mapping infrastructure to insulate and integrate these various components. This approach supports the flexible development of data standards and extensions, optimized storage backends, and data APIs, while allowing the other components of the data standards ecosystem to remain stable. To meet the demands of modern, large-scale science data, HDMF provides advanced data I/O functionality for iterative data write, lazy data load, and parallel I/O. It also supports optimization of data storage via support for chunking, compression, linking, and modular data storage. We demonstrate the application of HDMF in practice to design NWB 2.0 [13], a modern data standard for collaborative science across the neurophysiology community.},
  keywords = {Buildings,data formats,data modeling,data standards,HDF5,Memory,Metadata,neurophysiology,Standards organizations,Tools},
  file = {/Users/jonny/Zotero/storage/GT43JC2J/Tritt et al. - 2019 - HDMF Hierarchical Data Modeling Framework for Modern Science Data Standards.pdf}
}
